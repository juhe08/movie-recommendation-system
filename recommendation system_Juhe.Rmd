---
title: 'Capstone: Movie Recommendation System'
author: "Juhe Nie"
date: "2020/2/10"
output:
  pdf_document: default
  word_document: default
  html_document: default
---
# Executive Summary

The objective of this project is to create a recommendation system using the Movielens dataset. The Movielens dataset has been divided into two subsets “**edx**” and “**validation**”. We train and modify machine learning algorithm in the **edx** set and predict movie ratings in the **validation** set. We then split the edx dataset into a train set “**train_set**” and a test set “**test_set**”, where **train_set** is used to build algorithm and **test_set** is used to test. 

In our project, the root mean squared error (RMSE) is used as loss function. We start our prediction model with assuming the same rating for every data entry. We then use bias information of each user, movie, genre and time to learn the effect of users and three items (movie, genre and time) independently. To improve the results, we use regularization to penalize large estimates that come from small sample sizes. The best tuning parameter of regularization is equal to 5, which is chosen by using full-cross validation. After training and revising, we get RMSE of **test_set** as 0.864372. Finally, we use our recommendation model to predict ratings in the **validation** set and RMSE is **0.864568** (< 0.8649).

# Methods
```{r download data, include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
# if(!require(stringr))install.packages("stringr", repos = "http://cran.us.r-project.org")
if(! require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(! require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],title = as.character(title),genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

1. Basic information of edx dataset

At first, we need to observe some basic information from edx. Dataset edx has 6 objects (userId, movieId, rating, timestamp, title and genres) and 9,000,055 entries. Information from these objects will be used to create algorithm model and train data. 

```{r analyze edx, echo=TRUE, eval=FALSE}
edx_names <- names(edx)
edx %>% summarize (n_users=n_distinct(userId), n_movies = n_distinct(movieId))
```

2. Divide edx set

We then partition edx set into two subsets. We keep 95% of data in **train_set** to create and train the algorithm, and we use the other 5% of data to test the effect of our algorithm. This division ratio looks large, however, the edx set is a large dataset that has around nine million entries, and its **test_set** also has around 450 thousand entries which are enough to make a test. Therefore, this division is reasonable. 

```{r create train set and test set, echo=TRUE, message=FALSE}
set.seed(100)
edx_testindex <- createDataPartition(y = edx$rating, times = 1, p = 0.05, list = FALSE)
train_set <- edx[-edx_testindex,]
edx_temp <- edx[edx_testindex,]
test_set <- edx_temp %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
edx_removed <- anti_join(edx_temp, test_set)
train_set <- rbind(train_set, edx_removed)
```

3. RMSE

We use root mean squared error (RMSE) as our loss function to compare our prediction models to the true rating values. We denote N as the number of data entries used, $y_{u,i}$ as the rating for movie $i$ by user $u$, and $\hat{y}_{u,i}$ as our prediction, then RMSE is defined as follows: 

$$\sqrt{\frac{1}{N}\sum_{u,i}(\hat{y}_{u,i} - y_{u,i})^2}$$

We then create a function to calculate RMSE:
```{r RMSE, echo=TRUE, eval=TRUE}
RMSE <- function(true_ratings, predicted_ratings){sqrt(mean((true_ratings-predicted_ratings)^2))}
```

4. Model 1: Start with the average rating

To start with evaluating ratings, we firstly want to observe the overall rating distributions. As figure below shows, more ratings are clustered between 3 and 4. 

```{r number of ratings, echo=FALSE,fig.width=6, fig.height=4.5}
 nrating <- train_set %>% group_by(rating) %>% summarize(n=n())%>% .$n
 barplot(nrating,names.arg =  c("0.5","1","1.5","2","2.5","3","3.5","4","4.5","5"),col="black",xlab = "Rating scores",ylab = "The number of each rating")
 title(main = "Number of ratings versus ratings")
```

We then start with a model that assumes the same rating $\mu$ for all data entries, with all the differences explained by random variation $\epsilon_{u,i}$:

$$ y_{u,i}=\mu +\epsilon_{u,i} \tag{1} $$

In this project, the estimate of rating $\mu$, which can minimize the squared error, is the average rating across all use and the value of $\mu$ equals to 3.512452. This value is reasonable since it is in the range from 3 to 4. 

```{r include=FALSE}
mu <- mean(train_set$rating)
```

We regard Equation (1) as our naive model and use it to predict ratings in **test_set**. The RMSE result is 1.059933. 

```{r eval=FALSE, include=FALSE}
edx_rmse_1 <- RMSE(test_set$rating, test_set%>%mutate(newrating=mu)%>%.$newrating)
```

5. Model 2: Add specific effects of user, movie, date and genre

We then observe the effects of users, movies, dates and genres. In real life, some movies tend to have higher marks than average, while some movies tend to get more low rating scores. This can be proven by the figure below:


```{r movie_ratings, echo=FALSE,fig.width=6, fig.height=4.5}
train_set %>% group_by(movieId) %>% summarize(mr = (mean(rating)))%>% ungroup() %>%  ggplot(aes(mr)) + xlab("average rating for each movie") + geom_histogram(bins=30) + labs(title ="Average rating for each movie in train_set") + geom_vline(aes(xintercept = mu), color="red",linetype = "dashed")
```


This phenomenon leads to a movie bias between different movies, thus we add a new term $b_i$ to Equation (1) to represent the specific effect of each movie $i$. The value of $b_i$ is the mean value of the difference between true ratings of movie $i$ and $\mu$:

$$b_i = \frac{1}{n_i}\sum_{u=1}^{n_i}(y_{u,i}-\mu),$$

where $n_i$ is a number of rating for movie $i$. The code for calculating $b_i$ is 

```{r b_i, echo=TRUE,eval=FALSE}
movie_avgs <- train_set %>% group_by(movieId) %>% summarize(b_i=mean(rating-mu))
```

Due to the same reasons, we add a term $b_u$ for each user $u$ and a term $g_{u,i}$ for each genre combination. 


```{r,echo=TRUE, eval=FALSE}
user_avgs <- train_set %>% left_join(movie_avgs,by="movieId")%>% group_by(userId) %>% 
  summarize(b_u = mean(rating-mu - b_i))
genre_avgs <- train_set %>% left_join(movie_avgs,by="movieId") %>% 
  left_join(user_avgs,by = "userId") %>% group_by(genres) %>%
  summarize(g_ui = mean(rating- mu - b_i - b_u))
```


About the impact of time, we define $d_{u,i}$ as the timestamp for user's $u$ rating of movie $i$. We convert timestamp to a form of standard time and group them in months. As figure below shows, there is some evidence of a time effect on average rating. Therefore, we add a new function term $f(d_{u,i})$ to present the effect of each timestamp.


```{r date vs rating, echo=FALSE, message=FALSE,fig.width=6, fig.height=4.5}
train_set %>% mutate(date = as_datetime(timestamp)) %>% mutate(date = round_date(date, unit = "month")) %>%
       group_by(date) %>%
       summarize(rating = mean(rating)) %>%
       ggplot(aes(date, rating)) +
       geom_point() +
       geom_smooth() + labs(title = "Average rating change with months") + theme(plot.title = element_text(hjust = 0))
```


Taking effects of users, movies, dates and genres all into consideration, our model is written as:

$$ y_{u,i}=\mu +b_i+b_u+f(d_{u,i})+g_{u,i}+\epsilon_{u,i} $$

We use this model to predict rating in **test_set** again and get RMSE as 0.864962.

```{r eval=FALSE, include=FALSE}
movie_avgs <- train_set %>% group_by(movieId) %>% summarize(b_i=mean(rating-mu))
user_avgs <- train_set %>% left_join(movie_avgs,by="movieId")%>% group_by(userId) %>% summarize(b_u = mean(rating-mu - b_i))
genre_avgs <- train_set %>% left_join(movie_avgs,by="movieId") %>% left_join(user_avgs,by = "userId") %>% group_by(genres) %>% summarize(g_ui = mean(rating- mu - b_i - b_u))
date_avgs <- train_set %>% left_join(movie_avgs,by="movieId") %>% left_join(user_avgs,by = "userId") %>% left_join(genre_avgs, by = "genres") %>% mutate(date = as_datetime(timestamp))%>%mutate(date = round_date(date, unit = "month")) %>% group_by(date) %>% summarize(d_ui = mean(rating-mu-b_i-b_u-g_ui))
# calculate RMSE for test_set  
predicted_ratings <- test_set %>% mutate(date=as_datetime(timestamp))%>% mutate(date = round_date(date,unit="month"))%>% left_join(movie_avgs, by='movieId') %>% left_join(user_avgs, by='userId') %>% left_join(genre_avgs, by="genres") %>% left_join(date_avgs, by = "date") %>% mutate(pred = mu + b_i + b_u + g_ui + d_ui)%>%.$pred
edx_rmse_2 <- RMSE(predicted_ratings, test_set$rating)
```

6. Model 3: Regularization

To improve our results, we will use regularization. Regularization constrains the total variability of the effect sizes by penalizing large estimates that come from small sample sizes. By using regularization, the formula of $b_i$ is written as
$$ b_i = \frac{1}{n_i + \lambda}\sum_{u=1}^{n_i}(y_{u,i}-\mu),$$
where $\lambda$ is a tuning parameter. In the same way, we can get new expressions of $b_u$, $f(d_{u,i})$ and $g_{u,i}$.

We then use cross-validation to find the optimal $\lambda$. Figure below shows RMSE of test_set in edx when predicting with different values of $\lambda$ and we see that the root mean square error reaches its minimum value 0.864372 when $\lambda=5$. The establishment of algorithm model is done. 

```{r lambdas vs RMSE, echo=FALSE,fig.width=6, fig.height=4.5}
lambdas <- seq(0,10,1) 
 rmses <- sapply(lambdas,function(l){
 movie_avgs <- train_set %>% group_by(movieId) %>% summarize(b_i=sum(rating-mu)/(n()+l))
 user_avgs <- train_set %>% left_join(movie_avgs,by="movieId")%>% group_by(userId) %>% summarize(b_u = sum(rating-mu - b_i)/(n()+l))
 genre_avgs <- train_set %>% left_join(movie_avgs,by="movieId") %>% left_join(user_avgs,by = "userId") %>% group_by(genres) %>% summarize(g_ui = sum(rating- mu - b_i - b_u)/(n()+l))
 date_avgs <- train_set %>% left_join(movie_avgs,by="movieId") %>% left_join(user_avgs,by = "userId") %>% left_join(genre_avgs, by = "genres") %>% mutate(date = as_datetime(timestamp))%>%mutate(date = round_date(date, unit = "month")) %>% group_by(date) %>% summarize(d_ui = sum(rating-mu-b_i-b_u-g_ui)/(n()+l))
 predicted_ratings <- test_set %>% mutate(date=as_datetime(timestamp))%>% mutate(date = round_date(date,unit="month"))%>% left_join(movie_avgs, by='movieId') %>% left_join(user_avgs, by='userId') %>% left_join(genre_avgs, by="genres") %>% left_join(date_avgs, by = "date") %>% mutate(pred = mu + b_i + b_u + g_ui + d_ui) %>%.$pred
 return(RMSE(predicted_ratings, test_set$rating))})
d <- data.frame(lambdas = lambdas, rmses = rmses)
d %>% ggplot(aes(lambdas, rmses)) +
       geom_point() + labs(title = "RMSE versus lambdas") + theme(plot.title = element_text(hjust = 0))
```

# Results

Table below lists the RMSE result of predicting ratings in test_set of edx with different machine learning models. We can see significant improvements every time we modify the model. 

|Model |	RMSE |
|-----|-----|
| Naive model	| 1.059933 |
| Add bias effects |	0.864962 |
| Regularization | 	0.864372|

We finally use our algorithm to predict ratings of **validation** set and the RMSE value is **0.864568**, which is smaller than 0.8649. 

```{r eval=FALSE, include=FALSE}
lambda <- 5
movie_avgs <- train_set %>% group_by(movieId) %>% summarize(b_i=sum(rating-mu)/(n()+lambda))
 user_avgs <- train_set %>% left_join(movie_avgs,by="movieId")%>% group_by(userId) %>% summarize(b_u = sum(rating-mu - b_i)/(n()+lambda))
 genre_avgs <- train_set %>% left_join(movie_avgs,by="movieId") %>% left_join(user_avgs,by = "userId") %>% group_by(genres) %>% summarize(g_ui = sum(rating- mu - b_i - b_u)/(n()+lambda))
 date_avgs <- train_set %>% left_join(movie_avgs,by="movieId") %>% left_join(user_avgs,by = "userId") %>% left_join(genre_avgs, by = "genres") %>% mutate(date = as_datetime(timestamp))%>%mutate(date = round_date(date, unit = "month")) %>% group_by(date) %>% summarize(d_ui = sum(rating-mu-b_i-b_u-g_ui)/(n()+lambda))
valid_predicted <- validation %>% mutate(date=as_datetime(timestamp))%>% mutate(date = round_date(date,unit="month"))%>% left_join(movie_avgs, by='movieId') %>% left_join(user_avgs, by='userId') %>% left_join(genre_avgs, by="genres") %>% left_join(date_avgs, by = "date") %>% mutate(pred = mu + b_i + b_u + g_ui + d_ui) %>%.$pred
# valid_rmse is the final RMSE value
valid_rmse <- RMSE(valid_predicted, validation$rating)
```

# Conclusion

To create a movie recommendation system, we start with finding the mean rating value for all the data entries. We then study bias effect of users, movies, dates and genres and add these effects to the naïve model. To get a better result, we use regularization to modify the effect of each object. The best tuning parameter of regularization is chosen by using full-cross validation. In this project, the root mean squared error (RMSE) is used to compare the predicted results and true rating values. Finally, we predict ratings in the **validation** set and RMSE is **0.864568** (< 0.8649). 

The effect of each object in our model is studies independently. Our recommendation system can make further improvements by analyzing the correlations between different objects, such as analyzing the correlation between users and genres: whether a group of users tend to give similar scores for a type of genres and so on. 

